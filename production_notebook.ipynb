{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ccda0d-f8b1-47d8-b1aa-89397d918d41",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## What we tried to get a better accuracy \n",
    "1. take the log of the CustomerDistance so that outliers would not affect the accuracy as much \n",
    "    - This did not work \n",
    "    \n",
    "## Further developments\n",
    "1. Make the application object oriented i.e. put it into a class \n",
    "2. Provide a basic web app framework\n",
    "3. Feature Engineering\n",
    "4. KFolds Cross Validation\n",
    "5. Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef68b53",
   "metadata": {},
   "source": [
    "## 1) Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b435df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c19f8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5fc7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our modules \n",
    "from helper_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28f2be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Locate and Access Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf7eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_the_data_sets(store_data_path='./data/store.csv', data_path='./data/train.csv'):\n",
    "    # Read in store data\n",
    "    df_store = pd.read_csv(store_data_path, low_memory=False)\n",
    "\n",
    "    # Read in store data\n",
    "    df_train = pd.read_csv(data_path, low_memory=False)\n",
    "    \n",
    "    # merge the datasets\n",
    "    df_full = pd.merge(df_train, df_store, on=[\"Store\"])\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452a54f",
   "metadata": {},
   "source": [
    "## 3) Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc32426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "    # DROP COLUMNS\n",
    "    #     Date - Drop date because we have this information encoded in DaysOfWeek\n",
    "    #     CompetitionOpenSinceMonth, CompetitionOpenSinceYear - We think this information was encoded in the CompetitionDistance\n",
    "    #     Promo2SinceWeek, Promo2SinceYear, PromoInterval - Large percentage of nulls (47%) so not worth cutting all those rows or imputing\n",
    "    columns_to_remove = [\n",
    "        \"Date\", \n",
    "        \"CompetitionOpenSinceMonth\", \n",
    "        \"CompetitionOpenSinceYear\",\n",
    "        \"Promo2SinceWeek\",\n",
    "        \"Promo2SinceYear\",\n",
    "        \"PromoInterval\"\n",
    "    ]\n",
    "    df = df.drop(columns_to_remove, axis=1)\n",
    "    \n",
    "    # DROP ROWS\n",
    "    #     Sales - Cannot predict rows where there are null sales\n",
    "    #     Store - drop the nulls because we can't impute this\n",
    "    #     DayOfWeek - drop nulls because it's a small % of the dataset\n",
    "    #     Promo - Drop nulls for promo as they are a small percentage of the dataset\n",
    "    df = df.dropna(subset=[\"Sales\", \"Store\", \"DayOfWeek\", \"Promo\"])\n",
    "    \n",
    "    # Open - Drop closed days since sales should be 0 on these days\n",
    "    #      - Remove Open column aftwards as it now provides no more information\n",
    "    df = df.loc[df.loc[:, 'Open'] == 1]\n",
    "    df = df.drop('Open', axis=1)\n",
    "    \n",
    "    # Sales - Remove rows where sales are 0 as this breaks the RMSPE score\n",
    "    df = df.loc[df.loc[:, \"Sales\"] != 0]\n",
    "    \n",
    "    # IMPUTE DATA\n",
    "    # CompetitionDistance - Fill with the average\n",
    "    mean_competition_distance = df.loc[:, 'CompetitionDistance'].mean()\n",
    "    df.loc[:, \"CompetitionDistance\"] = df.loc[:, \"CompetitionDistance\"].fillna(mean_competition_distance)\n",
    "    \n",
    "    \"\"\"LOOK INTO THIS AS THIS IS ACTUALLY A BIT OF FEATURE ENGINEERING\"\"\"\n",
    "    # Customers - Fill with the average number of customers per store\n",
    "    #           - We do not know this information ahead of time so we need to use historical averages\n",
    "    #           - Drop the old Customers column after we add the new average customers per store column\n",
    "    mean_customers = df.groupby('Store')['Customers'].transform('mean').astype('int')\n",
    "    df.loc[:, 'average_customers_per_store'] = mean_customers\n",
    "    df = df.drop([\"Customers\"], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c1570-62b0-41d1-a82f-3be13d6669f9",
   "metadata": {},
   "source": [
    "## 4) Encode the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94937ed-efad-4716-98ea-ee0e78a991e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df):\n",
    "    \n",
    "    # Dummy Variables\n",
    "    #     StateHoliday\n",
    "    #     SchoolHoliday\n",
    "    #     DayOfWeek\n",
    "    #     StoreType\n",
    "    #     Assortment\n",
    "    dummy_columns = [\n",
    "        'StateHoliday',\n",
    "        'SchoolHoliday',\n",
    "        'DayOfWeek',\n",
    "        'StoreType',\n",
    "        'Assortment'\n",
    "    ]\n",
    "    dummy_column_names = [\n",
    "        'public_holiday',\n",
    "        'easter_holiday',\n",
    "        'christmas',\n",
    "        'not_school_holiday',\n",
    "        'school_holiday',\n",
    "        'monday',\n",
    "        'tuesday',\n",
    "        'wednesday',\n",
    "        'thursday',\n",
    "        'friday',\n",
    "        'saturday',\n",
    "        'sunday',\n",
    "        'store_model_1',\n",
    "        'store_model_2',\n",
    "        'store_model_3',\n",
    "        'store_model_4',\n",
    "        'basic',\n",
    "        'extra',\n",
    "        'extended'\n",
    "    ]\n",
    "    df_columns = [\n",
    "        'Store',\n",
    "        'Sales',\n",
    "        'Promo',\n",
    "        'CompetitionDistance',\n",
    "        'Promo2',\n",
    "        'average_customers_per_store'\n",
    "    ]\n",
    "    new_df_columns = df_columns + dummy_column_names\n",
    "    df = pd.get_dummies(data=df, columns=dummy_columns)\n",
    "    df = df.drop('StateHoliday_0', axis=1)\n",
    "    df.columns = new_df_columns\n",
    "    \n",
    "    # MEAN ENCODING\n",
    "    # STORE - add average sales per store\n",
    "    #       - remove Store column as it is now redundant\n",
    "    df.loc[:, 'average_sales_per_store'] = df.groupby('Store')['Sales'].transform('mean')\n",
    "    df = df.drop('Store', axis=1)\n",
    "    \n",
    "    # Finally reset the index after the data has been entirely transformed\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efaa00d-74b7-42f7-989b-ca322ae18716",
   "metadata": {},
   "source": [
    "## 5) Get Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dfae966-a607-4c7c-9743-9c6914cb6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(store_data_path='./data/store.csv', data_path='./data/train.csv'):\n",
    "    \n",
    "    # Load and merge the datasets \n",
    "    df = load_and_merge_the_data_sets(\n",
    "        store_data_path=store_data_path, \n",
    "        data_path=data_path\n",
    "    )\n",
    "    \n",
    "    # Clean the data\n",
    "    df_cleaned = clean_data(df)\n",
    "    \n",
    "    # Encode the data\n",
    "    df_encoded = encode_data(df_cleaned)\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f393732e-9f84-420f-bb4c-0dfb31b2dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_and_y_datasets(store_data_path='./data/store.csv', data_path='./data/train.csv'):\n",
    "    \n",
    "    # get the data\n",
    "    df = get_data(\n",
    "        store_data_path=store_data_path,\n",
    "        data_path=data_path\n",
    "    )\n",
    "    \n",
    "    # split into X and y \n",
    "    X = df.loc[:, df.columns.difference(['Sales'])]\n",
    "    y = df.loc[:, \"Sales\"]\n",
    "\n",
    "    return [X, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185d19a-59e2-4bbc-8de8-b905381dd552",
   "metadata": {},
   "source": [
    "## 6) Run Machine Learning Models Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90fb15ff-ad32-4991-9d7e-59a80178edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_mean_regressor(y_train, y_test):\n",
    "    # broadcast the mean predictions \n",
    "    mean_predictions = [y_train.mean()]\n",
    "    mean_predictions = np.array(mean_predictions * y_test.shape[0])\n",
    "    \n",
    "    return metric(mean_predictions, y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "107ad1de-dad4-4cd0-9da4-b5f581d2466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_linear_regression(X_train, y_train, X_test, y_test):\n",
    "    regressor = LinearRegression().fit(X_train, y_train)\n",
    "    linear_regression_predictions = regressor.predict(X_test)\n",
    "    \n",
    "    return metric(linear_regression_predictions, y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f43524-b3e3-44e5-939a-5aa17c8d9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_random_forest(X_train, y_train, X_test, y_test):\n",
    "    regressor_random_forest = RandomForestRegressor(n_estimators=40, min_samples_leaf=2, max_features=0.99, n_jobs=-1,oob_score=True)\n",
    "    regressor_random_forest.fit(X_train, y_train)\n",
    "    random_forest_predictions = regressor_random_forest.predict(X_test)\n",
    "    \n",
    "    return metric(random_forest_predictions, y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "169b3559-fe2d-4fa7-8d18-9512231a1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_xgboost(X_train, y_train, X_test, y_test):\n",
    "    regressor_xgboost = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "    regressor_xgboost.fit(X_train, y_train)\n",
    "    xgboost_predictions = regressor_xgboost.predict(X_test)\n",
    "    \n",
    "    return metric(xgboost_predictions, y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46514f9-8716-4169-b5b2-7e0a3c3d7950",
   "metadata": {},
   "source": [
    "## 7) Print Results Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "953dd277-96ec-447f-8165-1e9a860eff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    mean_regressor_metric = build_and_run_mean_regressor(y_train, y_test)\n",
    "    linear_regression_metric = build_and_run_linear_regression(X_train, y_train, X_test, y_test)\n",
    "    random_forest_metric = build_and_run_random_forest(X_train, y_train, X_test, y_test)\n",
    "    xgboost_metric = build_and_run_xgboost(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    return { \n",
    "        \"Mean Regressor\": mean_regressor_metric, \n",
    "        \"Linear Regression\": linear_regression_metric, \n",
    "        \"Random Forest\": random_forest_metric, \n",
    "        \"XGBoost\": xgboost_metric\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0365c62-9880-459f-8783-a5f6beefaaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_results(results):\n",
    "    for model, result in results.items():\n",
    "        percentage_result = round(result, 2)\n",
    "        print(f\"The RMSPE for the {model} model was {percentage_result}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd7671e-0f70-4232-b5c1-436d57ad258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models_and_print_results(test_data_path, train_data_path='./data/train.csv', store_data_path='./data/store.csv'):\n",
    "    \n",
    "    # load the train dataset\n",
    "    X_train, y_train = get_x_and_y_datasets(store_data_path=store_data_path, data_path=train_data_path)\n",
    "\n",
    "    # load the test dataset\n",
    "    X_test, y_test = get_x_and_y_datasets(store_data_path=store_data_path, data_path=test_data_path)\n",
    "\n",
    "    # get model results\n",
    "    model_results = get_model_results(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # print model results\n",
    "    print_model_results(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d33c6d8-d635-42e6-ba2e-01b1b4ee6e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSPE for the Mean Regressor model was 61.66%\n",
      "The RMSPE for the Linear Regression model was 29.15%\n",
      "The RMSPE for the Random Forest model was 20.55%\n",
      "The RMSPE for the XGBoost model was 21.47%\n"
     ]
    }
   ],
   "source": [
    "build_models_and_print_results(test_data_path='./data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d81a60-8d4e-4d5f-b811-97b783d6f9ec",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c6360-b186-4a5c-bc13-7f100651a866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac547e-399c-4b64-b369-b354a3c4287f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ee74e-fc46-4609-9eb4-e4d2b979574d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1859b-470f-4cd0-a620-3e9b5f58b341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc4157-6284-4542-80a8-708ae8a8f68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39271b07-4b95-4284-b126-5b8c6924f7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aab3def7-5da9-4bfb-a6ef-80e6a180e03c",
   "metadata": {},
   "source": [
    "# Rossman Production Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5516481-ac06-452e-b398-3eb93c9a69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Models \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Our modules \n",
    "from helper_methods import *\n",
    "\n",
    "class Rossman:\n",
    "    \n",
    "    def __init__(self, test_data_path, train_data_path='./data/train.csv', store_data_path='./data/store.csv'):\n",
    "        self.test_data_path=test_data_path\n",
    "        self.train_data_path=train_data_path\n",
    "        self.store_data_path=store_data_path\n",
    "        self.df = pd.DataFrame()\n",
    "        \n",
    "    # Locate and access the data\n",
    "    def load_and_merge_the_data_sets(self):\n",
    "        # Read in store data\n",
    "        df_store = pd.read_csv(self.store_data_path, low_memory=False)\n",
    "\n",
    "        # Read in store data\n",
    "        df_train = pd.read_csv(self.data_path, low_memory=False)\n",
    "\n",
    "        # merge the datasets\n",
    "        df_full = pd.merge(df_train, df_store, on=[\"Store\"])\n",
    "        \n",
    "        self.df = df_full\n",
    "    \n",
    "    # Data Cleaning\n",
    "    def clean_data(self):\n",
    "        \n",
    "        df = self.df\n",
    "\n",
    "        # DROP COLUMNS\n",
    "        #     Date - Drop date because we have this information encoded in DaysOfWeek\n",
    "        #     CompetitionOpenSinceMonth, CompetitionOpenSinceYear - We think this information was encoded in the CompetitionDistance\n",
    "        #     Promo2SinceWeek, Promo2SinceYear, PromoInterval - Large percentage of nulls (47%) so not worth cutting all those rows or imputing\n",
    "        columns_to_remove = [\n",
    "            \"Date\", \n",
    "            \"CompetitionOpenSinceMonth\", \n",
    "            \"CompetitionOpenSinceYear\",\n",
    "            \"Promo2SinceWeek\",\n",
    "            \"Promo2SinceYear\",\n",
    "            \"PromoInterval\"\n",
    "        ]\n",
    "        df = df.drop(columns_to_remove, axis=1)\n",
    "\n",
    "        # DROP ROWS\n",
    "        #     Sales - Cannot predict rows where there are null sales\n",
    "        #     Store - drop the nulls because we can't impute this\n",
    "        #     DayOfWeek - drop nulls because it's a small % of the dataset\n",
    "        #     Promo - Drop nulls for promo as they are a small percentage of the dataset\n",
    "        df = df.dropna(subset=[\"Sales\", \"Store\", \"DayOfWeek\", \"Promo\"])\n",
    "\n",
    "        # Open - Drop closed days since sales should be 0 on these days\n",
    "        #      - Remove Open column aftwards as it now provides no more information\n",
    "        df = df.loc[df.loc[:, 'Open'] == 1]\n",
    "        df = df.drop('Open', axis=1)\n",
    "\n",
    "        # Sales - Remove rows where sales are 0 as this breaks the RMSPE score\n",
    "        df = df.loc[df.loc[:, \"Sales\"] != 0]\n",
    "\n",
    "        # IMPUTE DATA\n",
    "        # CompetitionDistance - Fill with the average\n",
    "        mean_competition_distance = df.loc[:, 'CompetitionDistance'].mean()\n",
    "        df.loc[:, \"CompetitionDistance\"] = df.loc[:, \"CompetitionDistance\"].fillna(mean_competition_distance)\n",
    "\n",
    "        \"\"\"LOOK INTO THIS AS THIS IS ACTUALLY A BIT OF FEATURE ENGINEERING\"\"\"\n",
    "        # Customers - Fill with the average number of customers per store\n",
    "        #           - We do not know this information ahead of time so we need to use historical averages\n",
    "        #           - Drop the old Customers column after we add the new average customers per store column\n",
    "        mean_customers = df.groupby('Store')['Customers'].transform('mean').astype('int')\n",
    "        df.loc[:, 'average_customers_per_store'] = mean_customers\n",
    "        df = df.drop([\"Customers\"], axis=1)\n",
    "\n",
    "        self.df = df\n",
    "    \n",
    "    def encode_data(self):\n",
    "        \n",
    "        df = self.df\n",
    "\n",
    "        # Dummy Variables\n",
    "        #     StateHoliday\n",
    "        #     SchoolHoliday\n",
    "        #     DayOfWeek\n",
    "        #     StoreType\n",
    "        #     Assortment\n",
    "        dummy_columns = [\n",
    "            'StateHoliday',\n",
    "            'SchoolHoliday',\n",
    "            'DayOfWeek',\n",
    "            'StoreType',\n",
    "            'Assortment'\n",
    "        ]\n",
    "        dummy_column_names = [\n",
    "            'public_holiday',\n",
    "            'easter_holiday',\n",
    "            'christmas',\n",
    "            'not_school_holiday',\n",
    "            'school_holiday',\n",
    "            'monday',\n",
    "            'tuesday',\n",
    "            'wednesday',\n",
    "            'thursday',\n",
    "            'friday',\n",
    "            'saturday',\n",
    "            'sunday',\n",
    "            'store_model_1',\n",
    "            'store_model_2',\n",
    "            'store_model_3',\n",
    "            'store_model_4',\n",
    "            'basic',\n",
    "            'extra',\n",
    "            'extended'\n",
    "        ]\n",
    "        df_columns = [\n",
    "            'Store',\n",
    "            'Sales',\n",
    "            'Promo',\n",
    "            'CompetitionDistance',\n",
    "            'Promo2',\n",
    "            'average_customers_per_store'\n",
    "        ]\n",
    "        new_df_columns = df_columns + dummy_column_names\n",
    "        df = pd.get_dummies(data=df, columns=dummy_columns)\n",
    "        df = df.drop('StateHoliday_0', axis=1)\n",
    "        df.columns = new_df_columns\n",
    "\n",
    "        # MEAN ENCODING\n",
    "        # STORE - add average sales per store\n",
    "        #       - remove Store column as it is now redundant\n",
    "        df.loc[:, 'average_sales_per_store'] = df.groupby('Store')['Sales'].transform('mean')\n",
    "        df = df.drop('Store', axis=1)\n",
    "\n",
    "        # Finally reset the index after the data has been entirely transformed\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        self.df = df\n",
    "        \n",
    "    # Get Data Functions\n",
    "    def get_data(self):\n",
    "\n",
    "        # Load and merge the datasets \n",
    "        df = load_and_merge_the_data_sets(\n",
    "            store_data_path=self.store_data_path, \n",
    "            data_path=self.data_path\n",
    "        )\n",
    "\n",
    "        # Clean the data\n",
    "        df_cleaned = clean_data(df)\n",
    "\n",
    "        # Encode the data\n",
    "        df_encoded = encode_data(df_cleaned)\n",
    "\n",
    "        return df_encoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff162488-de51-42d1-b1a2-c83a926a279d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d872b-ffc8-4a1d-8a16-51e659ae3d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
